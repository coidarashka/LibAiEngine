name: Build AiEngine → Release

on:
  push:
    tags:
      - 'v*'
  workflow_dispatch:

env:
  NDK_VERSION: r26b
  NDK_HOME: ${{ github.workspace }}/../android-ndk
  PROJ: ${{ github.workspace }}/../project

jobs:
  build:
    name: Build libAiEngine.so (arm64-v8a)
    runs-on: ubuntu-22.04

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Create working directories
        run: mkdir -p $PROJ $PROJ/miniaudio $PROJ/stb

      - name: Install system dependencies
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y --no-install-recommends \
            build-essential cmake git wget unzip python3 binutils

      - name: Cache Android NDK
        id: cache-ndk
        uses: actions/cache@v4
        with:
          path: ${{ github.workspace }}/../android-ndk
          key: android-ndk-${{ env.NDK_VERSION }}

      - name: Download & install Android NDK ${{ env.NDK_VERSION }}
        if: steps.cache-ndk.outputs.cache-hit != 'true'
        run: |
          cd ${{ github.workspace }}/..
          wget -q https://dl.google.com/android/repository/android-ndk-${NDK_VERSION}-linux.zip
          unzip -q android-ndk-${NDK_VERSION}-linux.zip
          mv android-ndk-${NDK_VERSION} android-ndk
          rm android-ndk-${NDK_VERSION}-linux.zip

      - name: Remove dynamic libomp from NDK
        run: |
          find $NDK_HOME -name "libomp.so" -delete
          find $NDK_HOME -name "libomp.so.*" -delete

      - name: Clone llama.cpp
        run: git clone --depth 1 https://github.com/ggerganov/llama.cpp.git $PROJ/llama.cpp

      - name: Clone nlohmann/json
        run: git clone --depth 1 https://github.com/nlohmann/json.git $PROJ/json

      - name: Clone KleidiAI
        run: git clone --depth 1 https://github.com/ARM-software/kleidiai.git $PROJ/kleidiai_source

      - name: Download miniaudio & stb headers
        run: |
          wget -q https://raw.githubusercontent.com/mackron/miniaudio/master/miniaudio.h \
               -O $PROJ/miniaudio/miniaudio.h
          wget -q https://raw.githubusercontent.com/nothings/stb/master/stb_image.h \
               -O $PROJ/stb/stb_image.h
          wget -q https://raw.githubusercontent.com/nothings/stb/master/stb_image_write.h \
               -O $PROJ/stb/stb_image_write.h

      - name: Write bridge.cpp
        run: |
          cat > $PROJ/bridge.cpp << 'BRIDGE_EOF'
#include "llama.h"
#include "common.h"
#include "mtmd.h"
#include "mtmd-helper.h"
#include "nlohmann/json.hpp"
#include <android/log.h>
#include <string>
#include <vector>
#include <cstring>
#include <ctime>
#include <csignal>
#include <unistd.h>

using json = nlohmann::json;

#define TAG "MandreAI"
#define LOGD(...) __android_log_print(ANDROID_LOG_DEBUG, TAG, __VA_ARGS__)
#define LOGE(...) __android_log_print(ANDROID_LOG_ERROR, TAG, __VA_ARGS__)

struct GlobalConfig {
    int n_threads = 4;
    int n_threads_batch = 4;
    int n_ctx = 2048;
    int n_batch = 512;
    int img_max_tokens = 128;
    bool kv_quant = true;
};

static GlobalConfig g_conf;
static llama_model* g_model = nullptr;
static llama_context* g_ctx = nullptr;
static const llama_vocab* g_vocab = nullptr;
static llama_sampler* g_sampler = nullptr;
static mtmd_context* g_mtmd_ctx = nullptr;
static bool g_cancel_flag = false;

void signal_handler(int signum) {
    const char* sig_name = "UNKNOWN";
    switch(signum) {
        case SIGSEGV: sig_name = "SIGSEGV"; break;
        case SIGABRT: sig_name = "SIGABRT"; break;
        case SIGILL:  sig_name = "SIGILL";  break;
        case SIGFPE:  sig_name = "SIGFPE";  break;
    }
    LOGE("CRITICAL ENGINE CRASH: %s", sig_name);
    signal(signum, SIG_DFL);
    raise(signum);
}

extern "C" {
    int configure_engine(const char* json_str) {
        try {
            auto j = json::parse(json_str);
            if (j.contains("n_threads"))      g_conf.n_threads      = j["n_threads"];
            if (j.contains("n_ctx"))          g_conf.n_ctx          = j["n_ctx"];
            if (j.contains("n_batch"))        g_conf.n_batch        = j["n_batch"];
            if (j.contains("img_max_tokens")) g_conf.img_max_tokens = j["img_max_tokens"];
            LOGD("Config Applied: thr=%d, ctx=%d", g_conf.n_threads, g_conf.n_ctx);
            return 0;
        } catch (...) { LOGE("Config Parse Error!"); return -1; }
    }

    void register_crash_handlers() {
        signal(SIGSEGV, signal_handler); signal(SIGABRT, signal_handler);
        signal(SIGILL,  signal_handler); signal(SIGFPE,  signal_handler);
    }

    void set_inference_config(int sz) {}
    void cancel_inference() { g_cancel_flag = true; }

    int load_mmproj(const char* p) {
        if (!g_model) return -2;
        if (g_mtmd_ctx) mtmd_free(g_mtmd_ctx);
        mtmd_context_params params = mtmd_context_params_default();
        params.use_gpu = false;
        params.image_min_tokens = 32;
        params.image_max_tokens = g_conf.img_max_tokens;
        g_mtmd_ctx = mtmd_init_from_file(p, g_model, params);
        return g_mtmd_ctx ? 0 : -1;
    }

    int load_model(const char* p) {
        register_crash_handlers();
        llama_backend_init();
        llama_model_params mp = llama_model_default_params();
        mp.use_mmap = true;
        g_model = llama_model_load_from_file(p, mp);
        if (!g_model) return -1;
        g_vocab = llama_model_get_vocab(g_model);

        llama_context_params cp = llama_context_default_params();
        cp.n_ctx           = g_conf.n_ctx;
        cp.n_threads       = g_conf.n_threads;
        cp.n_threads_batch = g_conf.n_threads;
        cp.n_batch         = g_conf.n_batch;
        cp.n_ubatch        = g_conf.n_batch / 2;
        if (g_conf.kv_quant) {
            cp.type_k = GGML_TYPE_Q8_0;
            cp.type_v = GGML_TYPE_Q8_0;
        }

        g_ctx = llama_init_from_model(g_model, cp);
        if (!g_ctx) return -2;

        auto sp = llama_sampler_chain_default_params();
        g_sampler = llama_sampler_chain_init(sp);
        llama_sampler_chain_add(g_sampler, llama_sampler_init_penalties(64, 1.45f, 0.4f, 0.4f));
        llama_sampler_chain_add(g_sampler, llama_sampler_init_top_k(40));
        llama_sampler_chain_add(g_sampler, llama_sampler_init_top_p(0.95f, 1));
        llama_sampler_chain_add(g_sampler, llama_sampler_init_temp(0.7f));
        llama_sampler_chain_add(g_sampler, llama_sampler_init_dist((uint32_t)time(NULL)));
        LOGD("Engine Loaded. Kleidi: ON");
        return 0;
    }

    typedef void (*cb_t)(const char*);
    int infer(const char* pr, const char* img, cb_t cb) {
        if (!g_ctx || !g_sampler) return -1;
        g_cancel_flag = false;
        llama_memory_seq_rm(llama_get_memory(g_ctx), -1, -1, -1);

        if (img && g_mtmd_ctx && strlen(img) > 0) {
            long t0 = time(NULL);
            mtmd_bitmap* bmp = mtmd_helper_bitmap_init_from_file(g_mtmd_ctx, img);
            if (bmp) {
                std::string fp = std::string(mtmd_default_marker()) + "\n" + pr;
                mtmd_input_text it = {fp.c_str(), true, true};
                mtmd_input_chunks* ch = mtmd_input_chunks_init();
                const mtmd_bitmap* bmps[] = {bmp};
                if (mtmd_tokenize(g_mtmd_ctx, ch, &it, bmps, 1) == 0) {
                    llama_pos np = 0;
                    mtmd_helper_eval_chunks(g_mtmd_ctx, g_ctx, ch, 0, 0, g_conf.n_threads, true, &np);
                }
                LOGD("Vision Total: %ld seconds", time(NULL) - t0);
                mtmd_input_chunks_free(ch);
                mtmd_bitmap_free(bmp);
            }
        } else {
            std::vector<llama_token> tk(strlen(pr) + 16);
            int n = llama_tokenize(g_vocab, pr, strlen(pr), tk.data(), tk.size(), true, true);
            tk.resize(n);
            llama_decode(g_ctx, llama_batch_get_one(tk.data(), n));
        }

        for (int i = 0; i < 4096; i++) {
            if (g_cancel_flag) break;
            llama_token id = llama_sampler_sample(g_sampler, g_ctx, -1);
            if (llama_vocab_is_eog(g_vocab, id)) break;
            char b[256];
            int n = llama_token_to_piece(g_vocab, id, b, sizeof(b), 0, true);
            if (n > 0) cb(std::string(b, n).c_str());
            llama_sampler_accept(g_sampler, id);
            llama_batch batch = llama_batch_get_one(&id, 1);
            if (llama_decode(g_ctx, batch) != 0) break;
        }
        return 0;
    }

    void free_engine() {
        if (g_mtmd_ctx) mtmd_free(g_mtmd_ctx);
        if (g_sampler)  llama_sampler_free(g_sampler);
        if (g_model)    llama_model_free(g_model);
        if (g_ctx)      llama_free(g_ctx);
        llama_backend_free();
    }
}
BRIDGE_EOF

      - name: Write CMakeLists.txt
        run: |
          cat > $PROJ/CMakeLists.txt << 'CMAKE_EOF'
cmake_minimum_required(VERSION 3.10)
project(AiEngine)
set(CMAKE_CXX_STANDARD 17)

file(GLOB_RECURSE OMP_LIB "${NDK_HOME}/**/libomp.a")
list(GET OMP_LIB 0 OMP_PATH)

set(LLAMA_BUILD_COMMON ON CACHE BOOL "" FORCE)
set(GGML_CPU_KLEIDIAI  ON CACHE BOOL "" FORCE)

add_subdirectory(llama.cpp)

file(GLOB_RECURSE MTMD_H "llama.cpp/**/mtmd.h")
get_filename_component(MTD_DIR ${MTMD_H} DIRECTORY)

file(GLOB_RECURSE M_SRCS "llama.cpp/tools/mtmd/*.cpp")
list(FILTER M_SRCS EXCLUDE REGEX "mtmd-cli\\.cpp$|deprecation-warning\\.cpp$")
file(GLOB_RECURSE M_MOD  "llama.cpp/tools/mtmd/models/*.cpp")
file(GLOB_RECURSE M_HLP  "llama.cpp/tools/mtmd-helper.cpp")

add_library(AiEngine SHARED bridge.cpp ${M_SRCS} ${M_MOD} ${M_HLP})

target_link_libraries(AiEngine PRIVATE llama common log
    -Wl,--whole-archive "${OMP_PATH}" -Wl,--no-whole-archive)

target_include_directories(AiEngine PRIVATE
    llama.cpp/include llama.cpp/common
    llama.cpp/src llama.cpp/ggml/include
    json/include ${MTD_DIR} .)

target_link_options(AiEngine PRIVATE "-static-openmp" "-Wl,--exclude-libs,ALL")

target_compile_options(AiEngine PRIVATE
    -flto -funroll-loops -O3 -ffast-math -fopenmp -DGGML_FLASH_ATTN=ON)

target_link_options(AiEngine PRIVATE -flto)
CMAKE_EOF

      - name: CMake configure & build
        run: |
          mkdir -p $PROJ/build
          cd $PROJ/build
          cmake .. \
            -DCMAKE_TOOLCHAIN_FILE=$NDK_HOME/build/cmake/android.toolchain.cmake \
            -DANDROID_ABI=arm64-v8a \
            -DANDROID_PLATFORM=android-29 \
            -DANDROID_STL=c++_static \
            -DGGML_OPENMP=ON \
            -DGGML_FLASH_ATTN=ON \
            -DGGML_CPU_KLEIDIAI=ON \
            -DFETCHCONTENT_SOURCE_DIR_KLEIDIAI_DOWNLOAD=$PROJ/kleidiai_source \
            -DBUILD_SHARED_LIBS=OFF \
            -DNDK_HOME=$NDK_HOME
          make -j$(nproc) AiEngine

      - name: Verify no dynamic libomp
        run: |
          READELF=$NDK_HOME/toolchains/llvm/prebuilt/linux-x86_64/bin/llvm-readelf
          if $READELF -d $PROJ/build/libAiEngine.so | grep -q "libomp.so"; then
            echo "ERROR: libAiEngine.so dynamically links libomp.so!"
            exit 1
          fi
          echo "OK: No dynamic libomp"

      - name: Package artifact
        run: |
          mkdir -p ${{ github.workspace }}/dist
          cp $PROJ/build/libAiEngine.so ${{ github.workspace }}/dist/
          cd ${{ github.workspace }}/dist
          zip libAiEngine-arm64-v8a.zip libAiEngine.so

      - name: Upload build artifact
        uses: actions/upload-artifact@v4
        with:
          name: libAiEngine-arm64-v8a
          path: ${{ github.workspace }}/dist/libAiEngine-arm64-v8a.zip
          retention-days: 14

      - name: Create GitHub Release
        if: startsWith(github.ref, 'refs/tags/')
        uses: softprops/action-gh-release@v2
        with:
          name: "AiEngine ${{ github.ref_name }}"
          tag_name: ${{ github.ref_name }}
          body: |
            ## libAiEngine — ${{ github.ref_name }}

            | Файл | ABI | NDK | OpenMP |
            |------|-----|-----|--------|
            | `libAiEngine.so` | arm64-v8a | r26b | static |

            **Установка:** скопируйте `libAiEngine.so` в `src/main/jniLibs/arm64-v8a/`
          files: ${{ github.workspace }}/dist/libAiEngine-arm64-v8a.zip
          draft: false
          prerelease: false
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
